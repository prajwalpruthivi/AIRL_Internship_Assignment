{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e21175",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 0: installs & imports\n",
    "!pip install -q einops\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from einops import rearrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d29914",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 1: config, reproducibility, device\n",
    "cfg = {\n",
    "    \"seed\": 42,\n",
    "    \"epochs\": 12,\n",
    "    \"batch_size\": 128,\n",
    "    \"lr\": 3e-4,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"patch_size\": 4,\n",
    "    \"embed_dim\": 128,\n",
    "    \"depth\": 6,\n",
    "    \"num_heads\": 4,\n",
    "    \"mlp_ratio\": 4.0,\n",
    "    \"dropout\": 0.1,\n",
    "    \"num_classes\": 10\n",
    "}\n",
    "\n",
    "random.seed(cfg[\"seed\"])\n",
    "np.random.seed(cfg[\"seed\"])\n",
    "torch.manual_seed(cfg[\"seed\"])\n",
    "torch.cuda.manual_seed_all(cfg[\"seed\"])\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50680bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 2: CIFAR-10 data loaders\n",
    "mean = (0.4914, 0.4822, 0.4465)\n",
    "std = (0.2470, 0.2435, 0.2616)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "data_root = \"./data\"\n",
    "\n",
    "train_set = datasets.CIFAR10(root=data_root, train=True, download=True, transform=train_transform)\n",
    "test_set = datasets.CIFAR10(root=data_root, train=False, download=True, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=cfg[\"batch_size\"], shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=cfg[\"batch_size\"], shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(\"Train size:\", len(train_set), \"Test size:\", len(test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e3c27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 3: ViT implementation\n",
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_chans=3, embed_dim=128):\n",
    "        super().__init__()\n",
    "        assert img_size % patch_size == 0\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches_side = img_size // patch_size\n",
    "        self.n_patches = self.n_patches_side * self.n_patches_side\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, dropout=dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_norm = self.norm1(x)\n",
    "        attn_out, _ = self.attn(x_norm.permute(1,0,2), x_norm.permute(1,0,2), x_norm.permute(1,0,2))\n",
    "        attn_out = attn_out.permute(1,0,2)\n",
    "        x = x + attn_out\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class SimpleViT(nn.Module):\n",
    "    def __init__(self, img_size=32, patch_size=4, in_chans=3, num_classes=10, embed_dim=128, depth=6, num_heads=4, mlp_ratio=4., dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbed(img_size, patch_size, in_chans, embed_dim)\n",
    "        n_patches = self.patch_embed.n_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, n_patches + 1, embed_dim))\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "\n",
    "        mlp_dim = int(embed_dim * mlp_ratio)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(embed_dim, num_heads, mlp_dim, dropout) for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        cls_out = x[:, 0]\n",
    "        return self.head(cls_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94defb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 4: instantiate model + optimizer\n",
    "model = SimpleViT(img_size=32, patch_size=cfg[\"patch_size\"], embed_dim=cfg[\"embed_dim\"],\n",
    "                  depth=cfg[\"depth\"], num_heads=cfg[\"num_heads\"], mlp_ratio=cfg[\"mlp_ratio\"],\n",
    "                  num_classes=cfg[\"num_classes\"]).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=cfg[\"lr\"], weight_decay=cfg[\"weight_decay\"])\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg[\"epochs\"])\n",
    "\n",
    "def accuracy_from_logits(logits, labels):\n",
    "    preds = logits.argmax(dim=1)\n",
    "    return (preds == labels).float().mean().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d2187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 5: training loop\n",
    "best_acc = 0.0\n",
    "best_epoch = -1\n",
    "save_path = \"vit_cifar10_best.pth\"\n",
    "\n",
    "for epoch in range(1, cfg[\"epochs\"] + 1):\n",
    "    model.train()\n",
    "    running_loss, running_acc = 0.0, 0.0\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{cfg['epochs']} - train\", leave=False):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        running_acc += accuracy_from_logits(logits, labels) * imgs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_set)\n",
    "    epoch_acc = running_acc / len(train_set)\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    test_loss, test_acc = 0.0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in test_loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, labels)\n",
    "            test_loss += loss.item() * imgs.size(0)\n",
    "            test_acc += accuracy_from_logits(logits, labels) * imgs.size(0)\n",
    "\n",
    "    test_loss /= len(test_set)\n",
    "    test_acc /= len(test_set)\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"Epoch {epoch:02d} | Train acc: {epoch_acc*100:.2f}% | Test acc: {test_acc*100:.2f}%\")\n",
    "\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"New best model saved (epoch {epoch}) -> {best_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4f2aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cell 6: final evaluation\n",
    "model.load_state_dict(torch.load(save_path, map_location=device))\n",
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in test_loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        logits = model(imgs)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "final_acc = 100.0 * correct / total\n",
    "print(f\"Final Test Accuracy: {final_acc:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}